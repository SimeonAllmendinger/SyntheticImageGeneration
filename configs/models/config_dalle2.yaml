## Architecture

# CLIP
clip:
    #
    context_length: 77
    model_name: ViT-B-32              
    pretrained: laion2B-s34B-b79K     

# Diffusion Prior Network Parameters
diffusion_prior_network:
    params:
        dim: 512 
        depth: 6
        dim_head: 32 
        heads: 16 
        # Transformer: 32 x 16 = 512 (=dim), which equals the embed dim of clip

# Diffusion Prior Parameters
diffusion_prior:

    use_existing_model: True

    params:
        image_embed_dim: 512 
        timesteps: 1000
        sample_timesteps: 64
        cond_drop_prob: 0.2
        condition_on_text_encodings: False

# Diffusion Prior Trainer Parameters
diffusion_prior_trainer:

    model_save_path: ./src/assets/dalle2/models/DiffusionPriorTrainer_T45.pt
    train_with_embeds: True
    epochs: 10

    params:
        lr: 0.0003
        wd: 0.01
        ema_beta: 0.99
        ema_update_after_step: 1000
        ema_update_every: 10

# Decoder
decoder:

    use_existing_model: True

    params:
        image_sizes: 
            - 128
            - 224  
        timesteps: 1000
        image_cond_drop_prob: 0.1
        text_cond_drop_prob: 0.5

# Decoder Trainer
decoder_trainer:

    model_save_path: ./src/assets/dalle2/models/DecoderTrainer_T45.pt
    train_with_embeds: False

    params:
        lr: 0.00003
        wd: 0.01  # weight decay
        ema_beta: 0.99
        ema_update_after_step: 1000
        ema_update_every: 10

# unet 1
unet1:
    dim: 32
    image_embed_dim: 512    
    text_embed_dim: 512     
    cond_dim: 128
    channels: 3
    cond_on_text_encodings: True # set to True for any unets that need to be conditioned on text encodings
    dim_mults:
        - 1
        - 2
        - 4
        - 8

unet2:
    dim: 16
    image_embed_dim: 512   
    cond_dim: 128
    channels: 3
    dim_mults:
        - 1
        - 2
        - 4
        - 8
        - 16
